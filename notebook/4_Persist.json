{
	"name": "4_Persist",
	"properties": {
		"folder": {
			"name": "final"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "767f29b5-7e33-4837-9565-f6c41714031b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Persist()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Reading the dataframe\r\n",
					"\r\n",
					"df = spark.read.format('csv')\\\r\n",
					"                .option('header','true')\\\r\n",
					"                .load('abfss://optimization@projectsynapsestorage.dfs.core.windows.net/3. Cache Persist/cache.csv')"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"print('Usign with Column addign a column')\r\n",
					"from pyspark.sql.functions import col\r\n",
					"df_transform = df.withColumn('UnEmployed Rate Percentage',(col('Unemployed')/col('Labor Force'))*100)\r\n",
					"display(df_transform)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_dropped = df_transform.drop('Unemployment Rate')"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_dropped.printSchema()"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"df_converted = df_dropped.withColumn('Employed',F.col('Employed').cast('Integer'))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from datetime import date,datetime\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"start = datetime.now()\r\n",
					"\r\n",
					"\r\n",
					"# We are just doing a filtering columns where we are having degree\r\n",
					"df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
					"\r\n",
					"# Lets do a sorting on same dataframe\r\n",
					"df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
					"\r\n",
					"# Lets also do some grouping\r\n",
					"df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
					"\r\n",
					"\r\n",
					"end = datetime.now()\r\n",
					"\r\n",
					"duration = end-start\r\n",
					"\r\n",
					"print('Notebook executed in ' + str(duration))\r\n",
					""
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ==================== PERSIST - MEMORY ONLY -========================"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# PySpark Code : MEMORY_ONLY "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark import StorageLevel\r\n",
					"\r\n",
					"df_converted.persist(StorageLevel.MEMORY_ONLY)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Unpersist()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_converted.unpersist()"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Scala Code()\r\n",
					"# Persist MEMORY_ONLY"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"var df_Scala = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"abfss://optimization@projectsynapsestorage.dfs.core.windows.net/3. Cache Persist/cache.csv\")"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					" %%spark\r\n",
					"import org.apache.spark.storage.StorageLevel._ \r\n",
					"df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					" %%spark\r\n",
					"\r\n",
					"import org.apache.spark.sql.functions._\r\n",
					"\r\n",
					"df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
					"\r\n",
					"df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
					"\r\n",
					"df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark \r\n",
					"\r\n",
					"df_Scala.unpersist()"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Using Storage Class - MEMORY_ONLY using pyspark code"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### StorageLevel ( <-useDisk->,<-useMemory- >,<-useOffHeap->,<-de-serialized->,<-replication-> )\r\n",
					"### "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_converted.persist(StorageLevel(False,True,False,False,1))"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ==================== PERSIST - MEMORY AND DISK -========================"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# PySpark Code <br>\r\n",
					"# Persist - MEMORY AND DISK"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Unpersist previous cache or persisted StorageLevel\r\n",
					"\r\n",
					"df_converted.unpersist()\r\n",
					"\r\n",
					"# ------------------------------------ Storage Level -------------------------\r\n",
					"# Applying the  StorageLevel - MEMORY AND DISK\r\n",
					"\r\n",
					"from pyspark import StorageLevel\r\n",
					"\r\n",
					"df_converted.persist(StorageLevel.MEMORY_AND_DISK)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"# ----------------------------- ACTIONS -------------------------------\r\n",
					"\r\n",
					"from datetime import date,datetime\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"start = datetime.now()\r\n",
					"\r\n",
					"\r\n",
					"# We are just doing a filtering columns where we are having degree\r\n",
					"df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
					"\r\n",
					"# Lets do a sorting on same dataframe\r\n",
					"df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
					"\r\n",
					"# Lets also do some grouping\r\n",
					"df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
					"\r\n",
					"\r\n",
					"end = datetime.now()\r\n",
					"\r\n",
					"duration = end-start\r\n",
					"\r\n",
					"print('Notebook executed in ' + str(duration))"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_converted.unpersist()"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Scala Code <br>\r\n",
					"# Persist - MEMORY AND DISK"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"df_Scala.unpersist()\r\n",
					"\r\n",
					"// ---------------------------------- Storage Level --------------------\r\n",
					"\r\n",
					"\r\n",
					"import org.apache.spark.storage.StorageLevel._ \r\n",
					"df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK)\r\n",
					"\r\n",
					"// -------------------------------- Actions -------------------\r\n",
					"\r\n",
					"import org.apache.spark.sql.functions._\r\n",
					"\r\n",
					"df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
					"\r\n",
					"df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
					"\r\n",
					"df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ==================== PERSIST - MEMORY ONLY SER -========================"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Scala Code <br>\r\n",
					"# Persist - MEMORY_ONLY_SER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"df_Scala.unpersist()\r\n",
					"\r\n",
					"// ---------------------------------- Storage Level --------------------\r\n",
					"\r\n",
					"\r\n",
					"import org.apache.spark.storage.StorageLevel._ \r\n",
					"df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY_SER)\r\n",
					"\r\n",
					"// -------------------------------- Actions -------------------\r\n",
					"\r\n",
					"import org.apache.spark.sql.functions._\r\n",
					"\r\n",
					"df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
					"\r\n",
					"df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
					"\r\n",
					"df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ======================= PERSIST - MEMORY AND DISK SER -================="
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Scala Code\r\n",
					"# Persist - MEMORY AND DISK SER"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"df_Scala.unpersist()\r\n",
					"\r\n",
					"// ---------------------------------- Storage Level --------------------\r\n",
					"\r\n",
					"\r\n",
					"import org.apache.spark.storage.StorageLevel._ \r\n",
					"df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER)\r\n",
					"\r\n",
					"// -------------------------------- Actions -------------------\r\n",
					"\r\n",
					"import org.apache.spark.sql.functions._\r\n",
					"\r\n",
					"df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
					"\r\n",
					"df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
					"\r\n",
					"df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"df_Scala.unpersist()"
				],
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ======================= PERSIST - DISK ONLY -================="
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# PySpark Code <br>\r\n",
					"# Persist - DISK ONLY"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Unpersist previous cache or persisted StorageLevel\r\n",
					"\r\n",
					"df_converted.unpersist()\r\n",
					"\r\n",
					"# ------------------------------------ Storage Level -------------------------\r\n",
					"# Applying the  StorageLevel - MEMORY AND DISK\r\n",
					"\r\n",
					"from pyspark import StorageLevel\r\n",
					"\r\n",
					"df_converted.persist(StorageLevel.DISK_ONLY)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"# ----------------------------- ACTIONS -------------------------------\r\n",
					"\r\n",
					"from datetime import date,datetime\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"start = datetime.now()\r\n",
					"\r\n",
					"\r\n",
					"# We are just doing a filtering columns where we are having degree\r\n",
					"df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
					"\r\n",
					"# Lets do a sorting on same dataframe\r\n",
					"df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
					"\r\n",
					"# Lets also do some grouping\r\n",
					"df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
					"\r\n",
					"\r\n",
					"end = datetime.now()\r\n",
					"\r\n",
					"duration = end-start\r\n",
					"\r\n",
					"print('Notebook executed in ' + str(duration))"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_converted.unpersist()\r\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Scala Code <br>\r\n",
					"# Persist - DISK ONLY"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"df_Scala.unpersist()\r\n",
					"\r\n",
					"// ---------------------------------- Storage Level --------------------\r\n",
					"\r\n",
					"\r\n",
					"import org.apache.spark.storage.StorageLevel._ \r\n",
					"df_Scala.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)\r\n",
					"\r\n",
					"// -------------------------------- Actions -------------------\r\n",
					"\r\n",
					"import org.apache.spark.sql.functions._\r\n",
					"\r\n",
					"df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
					"\r\n",
					"df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
					"\r\n",
					"df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Scala Code <br>\r\n",
					"# Persist - OFF HEAP"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"df_Scala.unpersist()\r\n",
					"\r\n",
					"// ---------------------------------- Storage Level --------------------\r\n",
					"\r\n",
					"\r\n",
					"import org.apache.spark.storage.StorageLevel._ \r\n",
					"df_Scala.persist(org.apache.spark.storage.StorageLevel.OFF_HEAP)\r\n",
					"\r\n",
					"// -------------------------------- Actions -------------------\r\n",
					"\r\n",
					"import org.apache.spark.sql.functions._\r\n",
					"\r\n",
					"df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
					"\r\n",
					"df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
					"\r\n",
					"df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"df_Scala.unpersist()"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Pyspark Code <br>\r\n",
					"# Perist - MEMORY_ONLY_2"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Unpersist previous cache or persisted StorageLevel\r\n",
					"\r\n",
					"df_converted.unpersist()\r\n",
					"\r\n",
					"# ------------------------------------ Storage Level -------------------------\r\n",
					"# Applying the  StorageLevel - MEMORY AND DISK\r\n",
					"\r\n",
					"from pyspark import StorageLevel\r\n",
					"\r\n",
					"df_converted.persist(StorageLevel.MEMORY_ONLY_2)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"# ----------------------------- ACTIONS -------------------------------\r\n",
					"\r\n",
					"from datetime import date,datetime\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"start = datetime.now()\r\n",
					"\r\n",
					"\r\n",
					"# We are just doing a filtering columns where we are having degree\r\n",
					"df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
					"\r\n",
					"# Lets do a sorting on same dataframe\r\n",
					"df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
					"\r\n",
					"# Lets also do some grouping\r\n",
					"df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
					"\r\n",
					"\r\n",
					"end = datetime.now()\r\n",
					"\r\n",
					"duration = end-start\r\n",
					"\r\n",
					"print('Notebook executed in ' + str(duration))"
				],
				"execution_count": 1
			}
		]
	}
}