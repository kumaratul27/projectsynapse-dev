{
	"name": "2_Performing_Join_Transformation",
	"properties": {
		"folder": {
			"name": "section 13"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "5b8be826-8462-4fe7-b686-5d6306944a31"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"mssparkutils.notebook.run('4 - MSSpark Utilities/6 - Mount configuration')"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## - Accessing the Files from Mountpoint \r\n",
					"## Syntax:\r\n",
					"# synfs:/<jobid>/<mountpoint>/<path>\r\n",
					"# To get JobID - mssparkutils.env.getJobId()\r\n",
					"\r\n",
					"\r\n",
					"job_id = mssparkutils.env.getJobId()\r\n",
					"\r\n",
					"mount_point = 'synfs:/' + job_id + '/lake'"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Reading to join dataframe\r\n",
					"df_tojoin = spark.read.format('csv')\\\r\n",
					"                        .option('header','true')\\\r\n",
					"                        .load(mount_point+ '/For Joins/For Join Transformation/Education and Expected Salary ranges.csv')"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df_tojoin)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Reading transformed data from previous transformation\r\n",
					"df = spark.read.format('parquet')\\\r\n",
					"                        .load(mount_point+ '/DataTransformed/*.parquet')"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Applying join \r\n",
					"\r\n",
					"df_joined = df.join(df_tojoin,on='Education Level',how='inner')"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df_joined)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Writing transformed data to datalake\r\n",
					"\r\n",
					"df_joined.write.format('parquet')\\\r\n",
					"                .mode('overwrite')\\\r\n",
					"                .save(mount_point+'/JoinedData/')"
				],
				"execution_count": 12
			}
		]
	}
}